Namespace(backend='vllm', dataset=None, input_len=1024, output_len=1024, model='meta-llama/Meta-Llama-3.1-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, num_prompts=64, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=2048, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='cuda', num_scheduler_steps=1, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False, async_engine=False, disable_frontend_multiprocessing=False)
INFO 10-24 10:49:17 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 10-24 10:49:18 model_runner.py:1059] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 10-24 10:49:18 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.48it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.44it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.12it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.81it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.76it/s]

INFO 10-24 10:49:21 model_runner.py:1070] Loading model weights took 14.9888 GB
INFO 10-24 10:49:21 gpu_executor.py:122] # GPU blocks: 27984, # CPU blocks: 2048
INFO 10-24 10:49:21 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 218.62x
INFO 10-24 10:49:22 model_runner.py:1398] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 10-24 10:49:22 model_runner.py:1402] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 10-24 10:49:28 model_runner.py:1526] Graph capturing finished in 6 secs.
Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   2%|▏         | 1/64 [00:09<09:55,  9.46s/it, est. speed input: 108.26 toks/s, output: 108.26 toks/s]Processed prompts: 100%|██████████| 64/64 [00:09<00:00,  6.77it/s, est. speed input: 6928.35 toks/s, output: 6928.35 toks/s]
['First Token: bs == torch.Size([2048]), avg_time is 39.98742887503681 msecs, counts is 32\n']
['Next Token: bs == torch.Size([64]), avg_time is 7.10635245799951 msecs, counts is 1024\n']
['model fwd: bs == 2048, avg_time is 38.48389387130737 msecs, counts is 33\n', 'model fwd: bs == 64, avg_time is 4.9234520598940135 msecs, counts is 1024\n']
====== Warmup ======
Throughput: 6.74 requests/s, 
Total Throughput: 13804.68 tokens/s
Input Throughput: 6902.34 tokens/s
Output Throughput: 6902.34 tokens/s
Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   2%|▏         | 1/64 [00:09<10:10,  9.68s/it, est. speed input: 105.75 toks/s, output: 105.75 toks/s]Processed prompts: 100%|██████████| 64/64 [00:09<00:00,  6.61it/s, est. speed input: 6767.61 toks/s, output: 6767.60 toks/s]
['First Token: bs == torch.Size([2048]), avg_time is 40.23142040627192 msecs, counts is 32\n']
['Next Token: bs == torch.Size([64]), avg_time is 7.128545814453879 msecs, counts is 1024\n']
['model fwd: bs == 2048, avg_time is 38.92193357406124 msecs, counts is 32\n', 'model fwd: bs == 64, avg_time is 4.919673963026567 msecs, counts is 1024\n']
====== Warmup ======
Throughput: 6.58 requests/s, 
Total Throughput: 13485.21 tokens/s
Input Throughput: 6742.60 tokens/s
Output Throughput: 6742.60 tokens/s
Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   2%|▏         | 1/64 [00:09<10:09,  9.68s/it, est. speed input: 105.83 toks/s, output: 105.83 toks/s]Processed prompts: 100%|██████████| 64/64 [00:09<00:00,  6.61it/s, est. speed input: 6772.34 toks/s, output: 6772.33 toks/s]
['First Token: bs == torch.Size([2048]), avg_time is 40.26587096873868 msecs, counts is 32\n']
['Next Token: bs == torch.Size([64]), avg_time is 7.1190495146464805 msecs, counts is 1024\n']
['model fwd: bs == 2048, avg_time is 38.943054568383005 msecs, counts is 32\n', 'model fwd: bs == 64, avg_time is 4.920857087841015 msecs, counts is 1024\n']
====== Warmup ======
Throughput: 6.59 requests/s, 
Total Throughput: 13498.13 tokens/s
Input Throughput: 6749.07 tokens/s
Output Throughput: 6749.07 tokens/s
====== Final Result ======
Throughput: 6.59 requests/s, 
duration: 9.71 secs, inter-token latency: 9.48 msecs
Total Throughput: 13498.13 tokens/s
Input Throughput: 6749.07 tokens/s
Output Throughput: 6749.07 tokens/s
